{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:847908a040ead3ef9289b96754d54fbb40003facf39b2e97f5fb9acf7860091d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayes Parameter Estimation\n",
      "\n",
      "In the lecture, we have analyzed how the parameters of prior and posterior distributions relate analytically for Gaussian distributions. However, in the more general non-Gaussian case, the analysis is not possible. Instead, we need to use numerical methods to compute or estimate these distributions.\n",
      "\n",
      "Let us consider data generated by a distribution of parameter $\\theta$. The prior and the data-generating probability functions are the following:\n",
      "\n",
      "$$p(\\theta) = \\frac{1}{2} e^{-|\\theta|} \\qquad \\qquad p(x|\\theta) = \\frac{1}{2} e^{-|x-\\theta|}$$\n",
      "\n",
      "Given an observation $x$, the posterior distribution for the unkown parameter $\\theta$ can be obtained from the Bayes theorem:\n",
      "\n",
      "$$p(\\theta | x) = \\frac{p(\\theta) p(x | \\theta)}{p(x)} \\qquad \\text{where} \\quad p(x) = \\int p(\\theta) p(x | \\theta) d\\theta$$\n",
      "Suppose that we have observed the following data point:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = 3.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now study several techniques to compute/estimate this posterior distribution based either on explicit integration or sampling.\n",
      "\n",
      "### 1. Explicit Integration (10 P)\n",
      "\n",
      "Applying the Bayes formula requires the computation of an integral for $p(x)$. A numerical approximation of the integral is given by the Riemann sum:\n",
      "$$\n",
      "p(x) = \\sum_{k=-\\infty}^{\\infty} p(k \\cdot \\Delta\\theta) \\cdot p(x| k \\cdot \\Delta\\theta) \\cdot \\Delta\\theta\n",
      "$$\n",
      "where $\\theta = k \\cdot \\Delta \\theta$. In practice, we restrict the range of the integration where it has most of its support (i.e. choose k such that $\\theta$ is between $-10$ to $10$), and consider a small constant step size $\\Delta\\theta = 0.05$. In your code, you should use of numpy fast operations (e.g. `numpy.sum`) when possible, and avoid explicit loops in Python.\n",
      "\n",
      "* **Using the integration method, calculate the posterior function for the indicated range and step size.**\n",
      "* **Plot the prior and posterior probability functions.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "### REPLACE BY YOUR CODE\n",
      "import solution; solution.q1()\n",
      "###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. Basic Rejection Sampling (10 P)\n",
      "\n",
      "When the parameter space is large, analytic integration becomes untractable, and therefore, sampling algorithms are needed. In this exercise we test on the same one-dimensional example as in exercise 1, a simple rejection sampling algorithm to approximate the posterior distribution. Let $q(\\theta)$ be the joint probability function defined as $q(\\theta) = p(x|\\theta) \\cdot p(\\theta)$. The rejection sampling algorithm seeks to sample uniformly from the surface under the function $q(\\theta)$ by sampling uniformly from a larger box, and rejecting samples that are not under $q(\\theta)$. The algorithm operates as follows:\n",
      "\n",
      "repeat 100000 times:\n",
      "\n",
      "1. $\\theta \\sim \\mathcal{U}(-10,10)$\n",
      "2. $u \\sim \\mathcal{U}(0,1)$\n",
      "3. accept $\\theta$ if $u < q(\\theta)$\n",
      "\n",
      "where $\\mathcal{U}(a,b)$ denotes a uniform distribution on the interval $[a,b]$. The list of accepted $\\theta$s forms an empirical posterior distribution in the parameter space, that can be viewed as a probability function by computing a histogram. We use a bin size of 0.5 for the histogram.\n",
      "\n",
      "* **Implement this simple rejection sampling algorithm (make use of numpy parallelization when possible).**\n",
      "* **Create a histogram from the accepted $\\theta$s, and print the rejection rate.**\n",
      "* **Plot the (normalized) histogram in superposition to the functions of the previous exercise.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "### REPLACE BY YOUR CODE\n",
      "import solution; solution.q2()\n",
      "###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. Improving Rejection Sampling (10 P)\n",
      "\n",
      "As it could be seen in exercise 2, the estimation of the posterior function is quite noisy, because the high rejection rate strongly reduces the number of samples available to build the statistics. A simple technique to reduce the number of rejections is to define a smaller box $[-10,10] \\times [0,h]$, where $h$ is chosen as small as possible under the constraint that $h \\geq q(\\theta)$.\n",
      "\n",
      "* **Show that $q(\\theta)$ can be upper-bounded by $h = 0.25 e^{-|x|}$, and that the bound is tight.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[ REPLACE BY YOUR DEMONSTRATION ]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Perform the same experiment as in exercise 2, but using this tighter upper-bound for sampling.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "### REPLACE BY YOUR CODE\n",
      "import solution; solution.q3a()\n",
      "###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Show empirically, that if setting $h$ smaller (e.g. $h = 0.125 e^{-|x|}$), then the posterior is no longer sampled correctly.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "### REPLACE BY YOUR CODE\n",
      "import solution; solution.q3b()\n",
      "###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}