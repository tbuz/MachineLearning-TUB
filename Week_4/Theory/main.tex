%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{amssymb}

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

\newcommand{\PartDiv}[1]{\frac{\partial}{\partial #1}}

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Machine Learning 1 \\ Exercise 4} % Title

\author{Group: BSSBCH} % Author name

\date{\today} % Date for the report


\begin{document}

\maketitle % Insert the title, author and date
\noindent\rule[0.5ex]{\linewidth}{1pt}
Matthias Bigalke, 339547, maku@win.tu-berlin.de \\
Tolga Buz, 346836, buz\_tolga@yahoo.de \\
Alejandro Hernandez, 395678, alejandrohernandezmunuera@gmail.com \\
Aitor Palacios Cuesta, 396276, aitor.palacioscuesta@campus.tu-berlin.de \\
Christof Schubert, 344450, christof.schubert@campus.tu-berlin.de \\
Daniel Steinhaus, 342563, dany.steinhaus@googlemail.com\\
\noindent\rule[0.5ex]{\linewidth}{1pt}
% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Lagrange Multipliers}
\subsection*{(a)}
Find the parameter $\theta$ that minimizes $J(\theta)$ subject to the constraint $\theta^T b = 0$.\\ \\
We have
$$
\mathcal{L} = \sum \limits _{k=1}^n ||\boldsymbol{\theta} - \boldsymbol{x}_k||^2 + \lambda \boldsymbol{\theta}^T\boldsymbol{b}
$$
From this we get
\begin{align}
&0 = \PartDiv{\boldsymbol{\theta}} \mathcal{L}&\\
\Leftrightarrow~~&0 = \sum \limits _{k=1}^n 2(\boldsymbol{\theta} - \boldsymbol{x}_k) + \lambda \boldsymbol{b}&\\ 
\Leftrightarrow~~&0= \boldsymbol{\theta} - \overline{\boldsymbol{x}} + \frac{\lambda}{2n} \boldsymbol{b}&\\
\Leftrightarrow~~&\boldsymbol{\theta} = \overline{\boldsymbol{x}} - \frac{\lambda}{2n} \boldsymbol{b}&
\end{align}
and
\begin{align}
&0 = \PartDiv{\boldsymbol{\lambda}} \mathcal{L}&\\
\Leftrightarrow~~&0 =  \boldsymbol{\theta}^T \boldsymbol{b}&\\ 
\stackrel{(4)}{\Leftrightarrow}~~&0= (\overline{\boldsymbol{x}} - \frac{\lambda}{2n} \boldsymbol{b})^T \boldsymbol{b}&\\
\Leftrightarrow~~&0= \overline{\boldsymbol{x}}^T\boldsymbol{b} - \frac{\lambda}{2n} \boldsymbol{b}^T \boldsymbol{b}&\\
\stackrel{\boldsymbol{b} \neq \boldsymbol{0}}{\Leftrightarrow}~~&\lambda = 2n \frac {\overline{\boldsymbol{x}}^T\boldsymbol{b}}{\boldsymbol{b}^T \boldsymbol{b}}&\\
\end{align}
With (4) and (10) we finally get
$$
\boldsymbol{\theta} = \overline{\boldsymbol{x}} - \frac {\overline{\boldsymbol{x}}^T\boldsymbol{b}}{\boldsymbol{b}^T \boldsymbol{b}} \boldsymbol{b} 
=\overline{\boldsymbol{x}} - \frac {\overline{\boldsymbol{x}}^T\boldsymbol{b}}{||\boldsymbol{b}||} \cdot \frac{\boldsymbol{b}}{||\boldsymbol{b}||}  
$$
\subsubsection*{Geometrical interpretation}
$\frac {\overline{\boldsymbol{x}}^T\boldsymbol{b}}{||\boldsymbol{b}||}$ discribes a projection from $\boldsymbol{\overline{x}}$ on $\boldsymbol{b}$.
$\frac {\overline{\boldsymbol{x}}^T\boldsymbol{b}}{||\boldsymbol{b}||}\cdot \frac{\boldsymbol{b}}{||\boldsymbol{b}||}$ gives us the projection point. 
So this constraint gives us a minimum at the shifted emperical mean in opposite direction of $\boldsymbol{b}$ by the value of the projection.
\subsection*{(b)}
Find the parameter $\theta$ that minimizes $J(\theta)$ subject to the constraint $||\boldsymbol{\theta} - \boldsymbol{c}||^2 = 1 $.\\ \\
We have
$$
\mathcal{L} = \sum \limits _{k=1}^n ||\boldsymbol{\theta} - \boldsymbol{x}_k||^2 + \lambda ||\boldsymbol{\theta} - \boldsymbol{c}||^2 - \lambda
$$
From this we get
\begin{align}
                 &0 = \PartDiv{\theta} \mathcal{L}&\\
\Leftrightarrow~~&0 = \sum \limits _{k=1}^n 2\boldsymbol{\theta} - 2 \boldsymbol{x}_k + 2\lambda (\boldsymbol{\theta} - \boldsymbol{c})&\\
\Leftrightarrow~~&0 = n\boldsymbol{\theta} - \sum \limits _{k=1}^n  \boldsymbol{x}_k + \lambda \boldsymbol{\theta} - \lambda  \boldsymbol{c}&\\
\Leftrightarrow~~&0 = \boldsymbol{\theta} - \boldsymbol{\overline{x}} + \frac{\lambda}{n} \boldsymbol{\theta} - \frac{\lambda}{n}  \boldsymbol{c}&\\
\Leftrightarrow~~&\frac{n + \lambda}{n} \boldsymbol{\theta}  = \boldsymbol{\overline{x}} + \frac{\lambda}{n}  \boldsymbol{c}&\\
\Leftrightarrow~~&\boldsymbol{\theta}  = \frac{n}{n + \lambda}\boldsymbol{\overline{x}} + \frac{\lambda}{n + \lambda}  \boldsymbol{c}&
\end{align}
and
\begin{align}
                 &0 = \PartDiv{\lambda} \mathcal{L}&\\
\Leftrightarrow~~&0 = ||\boldsymbol{\theta} - \boldsymbol{c}||^2 - 1&\\
\Leftrightarrow~~&0 = ||\boldsymbol{\theta} - \boldsymbol{c}||^2 - 1& \text{eq (16)}\\            
\Leftrightarrow~~&0 = ||\frac{n}{n + \lambda}\boldsymbol{\overline{x}} + \frac{\lambda}{n + \lambda}  \boldsymbol{c} - \boldsymbol{c}||^2 - 1& \\
\Leftrightarrow~~&0 = ||\frac{n}{n + \lambda}\boldsymbol{\overline{x}} - \frac{n }{n + \lambda}  \boldsymbol{c} ||^2 - 1& \\
\Leftrightarrow~~&1 = \frac{n^2}{(n + \lambda})^2 ||\boldsymbol{\overline{x}} - \boldsymbol{c} ||^2&\\
\Leftrightarrow~~&(n + \lambda)^2  = n^2||\boldsymbol{\overline{x}} - \boldsymbol{c} ||^2&\\ 
\Rightarrow~~& \lambda  = n \cdot (||\boldsymbol{\overline{x}} - \boldsymbol{c} || - 1 )&
\end{align}
With Eq (16) and Eq (24) we get
\begin{align*}
&\boldsymbol{\theta}= \frac{n}{n + n ||\boldsymbol{\overline{x}} - \boldsymbol{c} || - n }\boldsymbol{\overline{x}} + \frac{n ||\boldsymbol{\overline{x}} - \boldsymbol{c} || - n}{n + n ||\boldsymbol{\overline{x}} - \boldsymbol{c} || - n}  \boldsymbol{c}&&\\
&~~= \frac{1}{||\boldsymbol{\overline{x}} - \boldsymbol{c} ||}\boldsymbol{\overline{x}} + \frac{||\boldsymbol{\overline{x}} - \boldsymbol{c} || - 1}{||\boldsymbol{\overline{x}} - \boldsymbol{c} ||}  \boldsymbol{c}&&\\
&~~= \frac{1}{||\boldsymbol{\overline{x}} - \boldsymbol{c} ||}\boldsymbol{\overline{x}}  - \frac{1}{||\boldsymbol{\overline{x}} - \boldsymbol{c} ||}  \boldsymbol{c} + c&&\\
&~~= \frac{1}{||\boldsymbol{\overline{x}} - \boldsymbol{c} ||}(\boldsymbol{\overline{x}}  - \boldsymbol{c}) + c&&\\
\end{align*}
\subsubsection*{Geometrical interpretation}
\subsubsection*{Geometrical interpretation}
The derived solution is as expected dependent on the vector $c$. In addition to the first case, $c$ has now not only additive impact but also anti-proportional, multiplicative impact
on $\overline{x}$. The presented solution is the optimum of $J$ considering the constraint $g$. At this point the contour line of J is tangential to the line describing g.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Bounds on Eigenvalues }
\subsection*{(a)}
Show that $\sum \limits _{i=1}^d S_{ii} \geq \lambda_1$.
\subsubsection*{Assumptions}
Since $S$ is a scatter matrix we can infer:
\begin{itemize}
	\item[(I)] $S$ is symmetrical 
	\item[(II)] $S$ is positive semi definite
	\item[(III)] The trace elements are positive, because we are using a correlation matrix
\end{itemize}
\subsubsection*{Proof}
We have
\begin{align*}
&\lambda_1 \leq \sum \limits _{i=1}^d S_{ii} = Tr(S) \stackrel{(I)}{=}
\sum \limits _{i=1}^d \lambda_i& \\
\Leftrightarrow~~& 0 \leq  \sum \limits _{i=2}^d \lambda_i&
\end{align*}
which is fullfilled because with (II) we get $\forall i \in \{1,...,d\} : \lambda_i \geq 0$.

\subsection*{(b)}
The bound becomes tight if the sum $\sum_{i=2}^d \lambda_i$ vanishes. This is the case if the data can be explained solely in terms of the first PC.

\subsection*{(c)}
In PCA, we maximize $\omega^T S \omega$ subject to the unit vector constraint. If we assume that the eigenvector $\omega_1$ is filled with zeroes at position $k$ (with $k$ corresponding to the elementary feature with largest variance), the first eigenvalue is then given by $\lambda_1=1\cdot var(X_k) = S_{kk} = max_{i=1}^d SS_{ii}$ (tight case). By using this method, if we get any other $\omega_1$ that combines multiple dimensions, $\lambda_1 \geq var(X_k) \geq S_{kk} = max_{i=1}^d SS_{ii}$ will hold instead.

\subsection*{(d)}
This bound becomes tight when the maximum variance lies on an elementary feature of the data, which means along one of the dimensions.


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{Iterative PCA}

\subsection*{(a)}
\begin{centering}
$\frac{\partial J(v)}{\partial v} = \frac{\partial J (\omega)}{\partial \omega} \cdot \frac{\partial \omega}{\partial v}$ \\
$\frac{\partial J (\omega)}{\partial \omega} = \frac{S^2 \omega}{||S \omega||} - S \omega $\\
$\omega = S^{-\frac{1}{2}} v$ \\
$\frac{\partial \omega}{\partial v} = S^{-\frac{1}{2}}$ \\
$\frac{\partial J (v)}{\partial v} = S^{-\frac{1}{2}} (\frac{S^2 \omega}{||S \omega||} - S \omega) $\\
\end{centering}



Inserting the partial derivative of $J$ (in respect to $v$) into the iteration step equation, we can write:

\begin{centering}
$S^{\frac{1}{2}} w{t+1} = S^{\frac{1}{2}} w_t + \gamma(S^{-\frac{1}{2}}(\frac{S^2 w}{||S w||} - S w)$ \\
$w_{t+1} = w_t + \gamma(S^{-\frac{1}{2}}(\frac{S^2 w}{||S w||} - S w)$

\end{centering}

Setting $\gamma=1$ yields: $w_{t+1} = \frac{Sw_t}{||Sw_t||}$

\subsection*{(b)}

We first set the cost function's gradient equal to zero and try to find the $w*$ that satisfies that equation: \\
\begin{centering}
 
$\frac{\partial J (w)}{\partial w} = \frac{S^2 w}{||Sw||} - Sw = 0$ \\
$\frac{Sw}{||Sw||} = w$ \\
$Sw = ||Sw|| w$
 
\end{centering}
 
As $||Sw||$ is a scaling factor of the vector $w$. we can apply the Euclidian norm on both sides and keep $||Sw||$ as a multiplicative factor: \\
$||Sw|| = ||Sw||\ ||w||$ \\
   
Obviously, this will only hold if $||w|| = 1$.
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\bibliographystyle{apalike}

%\bibliography{sample}

%----------------------------------------------------------------------------------------


\end{document}
