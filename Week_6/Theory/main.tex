%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{amssymb}
\usepackage{esvect}

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

\newcommand{\PartDiv}[1]{\frac{\partial}{\partial #1}}

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Machine Learning 1 \\ Exercise 6} % Title

\author{Group: BSSBCH} % Author name

\date{\today} % Date for the report


\begin{document}

\maketitle % Insert the title, author and date
\noindent\rule[0.5ex]{\linewidth}{1pt}
Matthias Bigalke, 339547, maku@win.tu-berlin.de \\
Tolga Buz, 346836, buz\_tolga@yahoo.de \\
Alejandro Hernandez, 395678, alejandrohernandezmunuera@gmail.com \\
Aitor Palacios Cuesta, 396276, aitor.palacioscuesta@campus.tu-berlin.de \\
Christof Schubert, 344450, christof.schubert@campus.tu-berlin.de \\
Daniel Steinhaus, 342563, dany.steinhaus@googlemail.com\\
\noindent\rule[0.5ex]{\linewidth}{1pt}
% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Finding the Direction of Maximal Correlation between Datasets}

\subsection*{a)}

It has to be maximized:

\begin{equation}
\begin{aligned}
argmax ~ corr(\vec{\omega}_x^Tx, \vec{\omega}_y^Ty) = \frac{cov( \vec{\omega}_x^Tx, \vec{\omega}_y^Ty)}{\sqrt{var( \vec{\omega}_x^Tx) var(\vec{\omega}_y^Ty)}}
\end{aligned}
\end{equation}

Maximizing the covariance is sufficient:

\begin{equation}
\begin{aligned}
argmax ~ cov( \vec{\omega}_x^Tx, \vec{\omega}_y^Ty) = argmax ~ \vec{\omega}_x^T S_{xy} \vec{\omega}_y
\end{aligned}
\end{equation}

with subject to:

\begin{equation}
\begin{aligned}
\vec{\omega}_x^T S_{xx} \vec{\omega}_x = 1\\
\vec{\omega}_y^T S_{yy} \vec{\omega}_y = 1\\
\end{aligned}
\end{equation}

(For $S$, $\vec{i}$ and $\vec{j}$: $S_{i, j} = \vec{i}^T\vec{j}$.)

\subsection*{b)}

Using the Lagrange-Function:

\begin{equation}
\begin{aligned}
L(\vec{\omega}_x, \vec{\omega}_y, \lambda_1, \lambda_2) = \vec{\omega}_x^T S_{xy}\vec{\omega}_y - \lambda_1(\vec{\omega}_x^T S_{xx}\vec{\omega}_x - 1) - \lambda_2 (\vec{\omega}_y^T S_{yy}\vec{\omega}_y - 1)
\end{aligned}
\end{equation}

To maximize for $\vec{\omega_x}$ and $\vec{\omega_y}$, the following equations have to be fulfilled:

\begin{align}
\frac{\partial L}{\partial \vec{\omega}_x} &\overset{!}{=} 0 = x^T y \vec{\omega}_y - \lambda_1(x^Tx\vec{\omega}_x)\\
\frac{\partial L}{\partial \vec{\omega}_y} &\overset{!}{=} 0 = (x^T y)^T \vec{\omega}_x - \lambda_2(y^Ty\vec{\omega}_y)
\end{align}

Equation (5) leads to

\begin{equation}
\begin{aligned}
&x^T y \vec{\omega}_y = \lambda_1(x^Tx\vec{\omega}_x) & ~~~| \cdot \vec{\omega}_x^T\\
\Leftrightarrow ~ & \vec{\omega}_x^T x^T y \vec{\omega}_y = \lambda_1(\vec{\omega}_x^T x^Tx\vec{\omega}_x) & ~~~| ~ with ~ \vec{\omega}_x^T x^Tx\vec{\omega}_x = 1 \\
\Leftrightarrow ~ &\lambda_1 = \vec{\omega}_x^T x^T y \vec{\omega}_y
\end{aligned}
\end{equation}

Equation (6) leads to:

\begin{equation}
\begin{aligned}
&(x^T y)^T \vec{\omega}_x = \lambda_2(y^Ty\vec{\omega}_y) & ~~~| \cdot \vec{\omega}_y^T\\
\Leftrightarrow ~ & \vec{\omega}_y^T(x^T y)^T \vec{\omega}_x = \lambda_2(\vec{\omega}_y^Ty^Ty\vec{\omega}_y) & ~~~| ~ with ~ \vec{\omega}_y^T y^Ty\vec{\omega}_y = 1 \\
\Leftrightarrow ~ &\lambda_2 = \vec{\omega}_y^T(x^T y)^T \vec{\omega}_x
\end{aligned}
\end{equation}

Since $\vec{\omega}_x^T x^T y \vec{\omega}_y = \vec{\omega}_y^T(x^T y)^T \vec{\omega}_x$ you have $\lambda_1 = \lambda_2 = \lambda$. \\
Take the initial equation from (7) and multiply with $(x^T x)^{-1}$:

\begin{equation}
\begin{aligned}
(x^T x)^{-1} (x^T y) \vec{\omega}_y = \lambda \vec{\omega}_x \\
\end{aligned}
\end{equation}

Take the initial equation from (8) and multiply with $(y^T y)^{-1}$:

\begin{equation}
\begin{aligned}
(y^T y)^{-1} (x^T y)^T \vec{\omega}_x = \lambda \vec{\omega}_y \\
\end{aligned}
\end{equation}

Replace $\vec{\omega}_y$ in equation (9) with the term from equation (10). This gives you the following eigenvalueproblem:

\begin{equation}
\begin{aligned}
(x^T x)^{-1} (x^T y) (y^T y)^{-1} (x^T y)^T \vec{\omega}_x = \lambda^2 \vec{\omega}_x \\
\end{aligned}
\end{equation}

Replace $\vec{\omega}_x$ in equation (10) with the term from equation (9). This gives you the following eigenvalueproblem:

\begin{equation}
\begin{aligned}
(y^T y)^{-1} (x^T y)^T (x^T x)^{-1} (x^T y) \vec{\omega}_y = \lambda^2 \vec{\omega}_y \\
\end{aligned}
\end{equation}

\subsection*{c)}

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Fisher and Bayes}

\subsection*{a)}

\subsection{b)}

We know that two classes are generated by two $d$-dimensional Gaussian distributions $p(x, \omega_1 ) \sim N (\mu_1 , \Sigma_1 )$ and $p(x, \omega_2 ) \sim N (\mu_2 , \Sigma_2 )$ with non-equal covariance matrices ($\Sigma_1 \neq \Sigma_2$).
The covariance matrices being non-equal means that the ratio between the likelihoods between both classes (which we use for classification) is not linear. Instead, we need to use a quadratic discriminant approach (QDA), as the decision boundaries are quadratic. \\ 

Then we can derive from the Bayes approach: \\ 
$b(x) = 1$ if $r_2^2 < r_1^2 + 2\ log \frac{P (\omega_1)}{P(\omega_2)} + log \frac{|\Sigma_1|}{|\Sigma_2|}$ \\
and $b(x) = 0$ otherwise. \\

In this case, $r_i^2 = (x − \mu_i )^T \Sigma_i^{−1} (x − \mu_i )$ with $i = 1, 2$ is the Mahalanobis distance. 
In analogy with the Fisher function, we can thus form the function: \\
$\psi (x) = r_2^2 - r_1^2 - log \frac{|\Sigma_1|}{|\Sigma_2|}$


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Programming}

See next page.
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\bibliographystyle{apalike}

%\bibliography{sample}

%----------------------------------------------------------------------------------------


\end{document}